# Cloud Data Warehouse & ETL Pipeline
## Project Overview
Sparkify is a music streaming company, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in [AWS S3](https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/API/API_Operations_Amazon_Simple_Storage_Service.html), in a directory of JSON logs on user activity on the music app, as well as a directory with JSON metadata on the songs in their app.

<div align="center">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/AWS.jpeg" width="50%">
</div>

As a data engineer, I will build an ETL pipeline that extracts their data from S3 and copy into staging table in [AWS Redshift](https://aws.amazon.com/redshift/getting-started/?p=rs&bttn=hero&exp=b), and transforms data into [Star Schema Database](https://www.databricks.com/glossary/star-schema#:~:text=A%20star%20schema%20is%20used,like%20transaction%20amounts%20and%20quantities). with a set of dimensional tables for marketing and analytics team to query aggregated information and find insights of song plays.


## AWS Redshift CPU Monitoring
<div align="center">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/EC2_nodes.png" width="100%">
</div>

## Dataset
There are 3 datasets that reside in S3:

Song data: `s3://udacity-dend/song_data`
Log data: `s3://udacity-dend/log_data`
log_json_path data: `s3://udacity-dend/log_json_path.json` contains the meta information that is required by AWS to correctly load `s3://udacity-dend/log_data`

#### Song Dataset: 
Song Dataset is a subset of real data from [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

For example, here are json file paths to two files in this dataset:

 - `song_data/A/B/C/TRABCEI128F424C983.json`
 - `song_data/A/A/B/TRAABJL12903CDCF1A.json`

An example of what a single song file:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

#### Log Dataset
The second dataset consists of log files in JSON format generated by this  [event simulator](https://github.com/Interana/eventsim)  based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings. The log files in the dataset are partitioned by year and month.

For example, here are file paths to two files in this dataset:

 - `log_data/2018/11/2018-11-12-events.json`
 - `log_data/2018/11/2018-11-13-events.json`

An example of what the data in a log file, 2018-11-12-events.json: 

<div align="center">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/log_data.png" width="80%">
</div>
 
## Data Warehouse Schema in Redshift for Song Play Analysis

<div align="center">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/Schema.png" width="80%">
</div>


## Instruction:
#### Set up AWS cloud cluster
 - Create IAM user, IAM role, VPC security group in AWS account
 - Attach policies including `AWSS3ReadOnlyAccess` and `AWSRedshiftFullAccess` to IAM role
 - Use AWS access key and secret key to create clients for `EC2`, `S3 bucket`, and `Redshift`.
 - Create a RedShift Cluster and get the Host address, database name, IAM user name and password 
 - Configure the file `dwh.cfg`
#### Create tables
 - `python create_tables_redshift.py`
#### ETL data
 - `python ETL_S3_Redshift.py`

## Example Query Result to Validate ETL pipeline
#### Query Analysis code is in [Data analysis](https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/data-analysis-queries.ipynb)

<div align="left">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/query_select.png" width="30%">
</div>


<div align="left">
  <img src="https://github.com/Ting-DS/Cloud-Data-Warehouse/blob/main/image/query_agg.png" width="30%">
</div>


## Project Steps

1. **Create Table Schemas**
   - Design table schemas and store SQL CREATE statements in `sql_queries.py`.
   - Use `create_tables.py` to connect to the database, create tables, and handle table drops if needed.
   - Set up an AWS Redshift cluster and create an IAM role with S3 read access.
   - Add Redshift database and IAM role info to `dwh.cfg`.
   - Test the schema creation with `create_tables.py`.

2. **Build ETL Pipeline**
   - In `etl.py`, implement data loading from S3 to staging tables and from staging to analytics tables in Redshift.
   - After running `create_tables.py`, execute `etl.py` to load and transform data.
   - Validate by running analytic queries on Redshift.

3. **Validate ETL Pipeline with Simple Query**
   - Write a basic query to retrieve data from analytics tables.
   - Execute the query on Redshift to verify ETL results.

4. **Remember to [delete your Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/managing-clusters-console.html#delete-cluster) cluster when done to prevent unnecessary costsÔºÅ**

Reference:
 - [AWS Redshift](https://aws.amazon.com/redshift/getting-started/?p=rs&bttn=hero&exp=b)
 - [AWS S3](https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/API/API_Operations_Amazon_Simple_Storage_Service.html)
 - [AWS EC2](https://docs.aws.amazon.com/en_us/ec2/)
